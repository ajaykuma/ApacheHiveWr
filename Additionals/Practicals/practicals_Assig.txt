1. Create a table in MySQL with columns to contain data from: https://github.com/ajaykuma/Datasets/blob/master/auction.csv .
This table shouldn’t have any primary key. Load this data into the MySQL table. Now using Sqoop , import this data into a hive table (tbl1) without the table already existing in Hive. Also this load should use multiple mappers (for ex: 4) Create a partitioned table (tbl2) that will contain data from “sqoop import created table”. We want to use “dynamic partitioning” to load data into tbl2 and let the partitions get created and loaded automatically. Note** choose column appropriately to partition data in tbl2 , so that you don’t end up with many partitions.
2. Create a partitioned table in Hive containing columns with primitive and complex data types (array, map & struct). Let the table support default delimiters. Load data into this table.
Query the table to trigger a mapreduce job that extracts (1 column with primitive data type, 1st and 3rd index element from array column, 1st struct element, map column), orders this data in desc order.
3. Create a table from an existing table that contains data (from kv1.txt file from git link or from /usr/local/hive/examples/files ). The new table should have its data in compressed format. Choose the best compression scheme and mention why you choose it.
4. Demonstrate your knowledge of using an in-built function and UDF while querying data from a table containing primitive and complex data types.
5. Create a table pointing to a directory on HDFS that doesn’t contain any data. Load data into this table without using HDFS commands or Hive’s load command. Hint: Use Data Ingestion Tool/Any other MapReduce Approach to get results in shorter duration.